<div align="center">

<h2>
<strong>How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning</strong>
</h2>

<h4>ğŸ” We introduce <strong>find-embedding</strong> with <strong>GG-SM</strong>: enabling decoder-only LLMs to holistically perceive users via adaptive attention masking ğŸ¯</h4>

[![Paper](https://img.shields.io/badge/arXiv-2512.01282-b31b1b.svg)](https://arxiv.org/abs/2512.01282)
![GitHub Repo stars](https://img.shields.io/github/stars/JhCircle/Deepfind-GGSM?style=social)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-daily--paper-orange.svg)](https://huggingface.co/datasets/Jhcircle/KadiaBench)

</div>

<p align="center"><strong>ğŸ“§ Contact:</strong> <a href="mailto:jhyuan.cs@gmail.com">jhyuan.cs@gmail.com</a> 
<p align="center">
<img src="./assets/Deepfind-GGSM.png" align="center" width="90%">
</p>

---

## ğŸ”¥ News
* `2026.02` ğŸ‰ Our paper [*How Do Decoder-Only LLMs Perceive Users?*](https://arxiv.org/abs/2512.01282) has been released on arXiv â€” check it out now!  
  > ğŸ’» **Code is available at [https://github.com/JhCircle/Deepfind-GGSM](https://github.com/JhCircle/Deepfind-GGSM)**

---

## âš–ï¸ License
This project is licensed under the **Apache License Version 2.0**. See the [LICENSE](./LICENSE) file for details.